{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36ded46a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: torch.Size([49999, 3, 32, 32])\n",
      "Training labels shape: torch.Size([49999])\n",
      "Test data shape: torch.Size([10000, 3, 32, 32])\n",
      "Test labels shape: torch.Size([10000])\n"
     ]
    }
   ],
   "source": [
    "# CIFAR Dataset Preprocessing \n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import torch.nn.functional as F\n",
    "\n",
    "DATA_ROOT = r'C:\\Users\\Akshay Patil\\Desktop\\cifar-10-python\\cifar-10-batches-py'\n",
    "CIFAR_MEAN = (0.4914, 0.4822, 0.4465)\n",
    "CIFAR_STD = (0.2470, 0.2435, 0.2616)\n",
    "\n",
    "def load_cifar_batch(file_path):\n",
    "    \"\"\"Load CIFAR-10 batch from local file\"\"\"\n",
    "    with open(file_path, 'rb') as f:\n",
    "        batch = pickle.load(f, encoding='latin1')\n",
    "    images = batch['data'].reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1)\n",
    "    labels = batch['labels']\n",
    "    return images, labels\n",
    "\n",
    "class TestDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Custom dataset to load test images in original order\"\"\"\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = Path(root_dir)\n",
    "        self.transform = transform\n",
    "        self.image_files = sorted(self.root_dir.glob('*.png'), \n",
    "                                 key=lambda x: int(x.stem.split('_')[1]))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_files[idx]\n",
    "        image = Image.open(img_path)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "def generate_sample_data():\n",
    "    # Define transforms with normalization\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(CIFAR_MEAN, CIFAR_STD)\n",
    "    ])\n",
    "    \n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(CIFAR_MEAN, CIFAR_STD)\n",
    "    ])\n",
    "\n",
    "    trainset = torchvision.datasets.ImageFolder(\n",
    "        root=str(Path(DATA_ROOT) / 'train_organized'),\n",
    "        transform=train_transform\n",
    "    )\n",
    "    trainloader = DataLoader(trainset, batch_size=60000, shuffle=False)\n",
    "    input_images, class_labels = next(iter(trainloader))\n",
    "    \n",
    "    testset = TestDataset(\n",
    "        root_dir=Path(DATA_ROOT) / 'test_original',\n",
    "        transform=test_transform\n",
    "    )\n",
    "    testloader = DataLoader(testset, batch_size=10000, shuffle=False)\n",
    "    test_images = next(iter(testloader))\n",
    "    \n",
    "    _, test_labels = load_cifar_batch(Path(DATA_ROOT) / 'test_batch')\n",
    "    test_labels = torch.tensor(test_labels, dtype=torch.long)\n",
    "    \n",
    "    return input_images, class_labels, test_images, test_labels\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train_images, train_labels, test_images, test_labels = generate_sample_data()\n",
    "    \n",
    "    print(f\"Training data shape: {train_images.shape}\")\n",
    "    print(f\"Training labels shape: {train_labels.shape}\")\n",
    "    print(f\"Test data shape: {test_images.shape}\")\n",
    "    print(f\"Test labels shape: {test_labels.shape}\")\n",
    "    train_images = train_images.reshape(49999,3,32,32)\n",
    "    test_images = test_images.reshape(10000,3,32,32)\n",
    "    train_images = F.normalize(train_images,p=2,dim=1)\n",
    "    test_images = F.normalize(test_images,p=2,dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af981947",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Akshay Patil\\AppData\\Local\\Temp\\ipykernel_1944\\2927765739.py:88: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_vectors = torch.tensor(train_images, dtype=torch.float64)\n",
      "C:\\Users\\Akshay Patil\\AppData\\Local\\Temp\\ipykernel_1944\\2927765739.py:95: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(test_images, dtype=torch.float64),\n",
      "C:\\Users\\Akshay Patil\\AppData\\Local\\Temp\\ipykernel_1944\\2927765739.py:96: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(test_labels, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "Train Loss: 2.2731 | Acc: 0.2192\n",
      "Test Loss: 2.2382 | Acc: 0.3354\n",
      "--------------------------------------------------\n",
      "Epoch 2/100\n",
      "Train Loss: 2.2167 | Acc: 0.3728\n",
      "Test Loss: 2.1940 | Acc: 0.4013\n",
      "--------------------------------------------------\n",
      "Epoch 3/100\n",
      "Train Loss: 2.1734 | Acc: 0.4229\n",
      "Test Loss: 2.1531 | Acc: 0.4369\n",
      "--------------------------------------------------\n",
      "Epoch 4/100\n",
      "Train Loss: 2.1284 | Acc: 0.4583\n",
      "Test Loss: 2.1086 | Acc: 0.4623\n",
      "--------------------------------------------------\n",
      "Epoch 5/100\n",
      "Train Loss: 2.0826 | Acc: 0.4805\n",
      "Test Loss: 2.0687 | Acc: 0.4732\n",
      "--------------------------------------------------\n",
      "Epoch 6/100\n",
      "Train Loss: 2.0349 | Acc: 0.4989\n",
      "Test Loss: 2.0275 | Acc: 0.4816\n",
      "--------------------------------------------------\n",
      "Epoch 7/100\n",
      "Train Loss: 1.9849 | Acc: 0.5159\n",
      "Test Loss: 1.9818 | Acc: 0.4858\n",
      "--------------------------------------------------\n",
      "Epoch 8/100\n",
      "Train Loss: 1.9332 | Acc: 0.5261\n",
      "Test Loss: 1.9420 | Acc: 0.4958\n",
      "--------------------------------------------------\n",
      "Epoch 9/100\n",
      "Train Loss: 1.8809 | Acc: 0.5395\n",
      "Test Loss: 1.9020 | Acc: 0.4883\n",
      "--------------------------------------------------\n",
      "Epoch 10/100\n",
      "Train Loss: 1.8258 | Acc: 0.5526\n",
      "Test Loss: 1.8460 | Acc: 0.5101\n",
      "--------------------------------------------------\n",
      "Epoch 11/100\n",
      "Train Loss: 1.7686 | Acc: 0.5668\n",
      "Test Loss: 1.8184 | Acc: 0.4932\n",
      "--------------------------------------------------\n",
      "Epoch 12/100\n",
      "Train Loss: 1.7147 | Acc: 0.5742\n",
      "Test Loss: 1.7726 | Acc: 0.5089\n",
      "--------------------------------------------------\n",
      "Epoch 13/100\n",
      "Train Loss: 1.6469 | Acc: 0.5982\n",
      "Test Loss: 1.7189 | Acc: 0.5199\n",
      "--------------------------------------------------\n",
      "Epoch 14/100\n",
      "Train Loss: 1.5830 | Acc: 0.6101\n",
      "Test Loss: 1.6862 | Acc: 0.5216\n",
      "--------------------------------------------------\n",
      "Epoch 15/100\n",
      "Train Loss: 1.5273 | Acc: 0.6199\n",
      "Test Loss: 1.6496 | Acc: 0.5210\n",
      "--------------------------------------------------\n",
      "Epoch 16/100\n",
      "Train Loss: 1.4619 | Acc: 0.6318\n",
      "Test Loss: 1.6059 | Acc: 0.5222\n",
      "--------------------------------------------------\n",
      "Epoch 17/100\n",
      "Train Loss: 1.4003 | Acc: 0.6418\n",
      "Test Loss: 1.5713 | Acc: 0.5283\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Chebyshev NN for CIFAR (Same Width across all layers)\n",
    "# Architechture can be made more in depth...\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class SharedChebyshevLayer(nn.Module):\n",
    "    def __init__(self, degree):\n",
    "        super().__init__()\n",
    "        self.degree = degree\n",
    "        self.coeffs = nn.Parameter(torch.randn(3072, degree+1, dtype=torch.float64))\n",
    "        \n",
    "    def _chebyshev_basis(self, x):\n",
    "        \"\"\"Inplace-operation-free Chebyshev polynomial basis computation\"\"\"\n",
    "        batch_size, feat_dim = x.shape\n",
    "        x_flat = x.reshape(-1)\n",
    "        \n",
    "        T_list = [torch.ones_like(x_flat)]\n",
    "        if self.degree >= 1:\n",
    "            T_list.append(x_flat)\n",
    "            for k in range(1, self.degree):\n",
    "                Tk = 2 * x_flat * T_list[k] - T_list[k-1]\n",
    "                T_list.append(Tk)\n",
    "\n",
    "        T = torch.stack(T_list, dim=1)\n",
    "        return T.view(batch_size, feat_dim, self.degree+1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        basis = self._chebyshev_basis(x)\n",
    "        return torch.einsum('bik,ik->bi', basis, self.coeffs)\n",
    "\n",
    "class ChebyshevSharedNN(nn.Module):\n",
    "    def __init__(self, num_layers=3, cheb_degree=10, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.cheb_degree = cheb_degree\n",
    "        self.num_classes = num_classes\n",
    "        self.init_transform = nn.Linear(3072, 3072, dtype=torch.float64)\n",
    "        self.transforms = nn.ModuleList([\n",
    "            nn.Linear(3072, 3072, dtype=torch.float64) \n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.cheb_layers = nn.ModuleList([\n",
    "            SharedChebyshevLayer(cheb_degree)\n",
    "            for _ in range(num_layers+1)\n",
    "        ])\n",
    "        self.class_projections = nn.ModuleList([\n",
    "            nn.Linear(3072, 1, dtype=torch.float64)\n",
    "            for _ in range(num_classes)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(torch.float64)\n",
    "        x = F.normalize(self.init_transform(x), p=2.0, dim=1)\n",
    "        for transform, cheb_layer in zip(self.transforms, self.cheb_layers[0:3]):\n",
    "            modulated = cheb_layer(x)\n",
    "            x = F.normalize(transform(modulated), p=2.0, dim=1)\n",
    "        x = self.cheb_layers[3](x)\n",
    "        x = F.normalize(x,p = 2.0 , dim = 1)\n",
    "        return torch.cat([proj(x) for proj in self.class_projections], dim=1)\n",
    "\n",
    "\n",
    "\n",
    "input_vectors = torch.tensor(train_images, dtype=torch.float64)\n",
    "target_labels = train_labels \n",
    "train_dataset = TensorDataset(input_vectors, target_labels)\n",
    "test_dataset = TensorDataset(\n",
    "    torch.tensor(test_images, dtype=torch.float64),\n",
    "    torch.tensor(test_labels, dtype=torch.long)\n",
    ")\n",
    "train_loader = DataLoader(train_dataset, batch_size=1000, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
    "\n",
    "model = ChebyshevSharedNN(num_layers=3, cheb_degree=20).to(device) # hyperparmeter degree of the series , based on this we can create a lesser depth circuit by giving more degrees of freedom to the activation function tuned to the dataset...\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=2, factor=0.5)\n",
    "\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    train_loss, train_correct = 0.0, 0\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item() * inputs.size(0)\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        train_correct += (preds == labels).sum().item()\n",
    "    \n",
    "    model.eval()\n",
    "    test_loss, test_correct = 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            test_loss += criterion(outputs, labels).item() * inputs.size(0)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            test_correct += (preds == labels).sum().item()\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    train_acc = train_correct / len(train_loader.dataset)\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_acc = test_correct / len(test_loader.dataset)\n",
    "    \n",
    "    # Update learning rate\n",
    "    scheduler.step(test_loss)\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/100')\n",
    "    print(f'Train Loss: {train_loss:.4f} | Acc: {train_acc:.4f}')\n",
    "    print(f'Test Loss: {test_loss:.4f} | Acc: {test_acc:.4f}')\n",
    "    print('-'*50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f579e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Akshay Patil\\AppData\\Local\\Temp\\ipykernel_6852\\662598423.py:112: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_vectors = torch.tensor(train_images, dtype=torch.float64)\n",
      "C:\\Users\\Akshay Patil\\AppData\\Local\\Temp\\ipykernel_6852\\662598423.py:113: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  target_labels = torch.tensor(train_labels, dtype=torch.long)\n",
      "C:\\Users\\Akshay Patil\\AppData\\Local\\Temp\\ipykernel_6852\\662598423.py:117: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(test_images, dtype=torch.float64),\n",
      "C:\\Users\\Akshay Patil\\AppData\\Local\\Temp\\ipykernel_6852\\662598423.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(test_labels, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1/1000 | Train: loss 2.3040, acc 0.1009 | Val: loss 2.3015, acc 0.1155\n",
      "Epoch   2/1000 | Train: loss 2.2808, acc 0.1263 | Val: loss 2.2460, acc 0.1960\n",
      "Epoch   3/1000 | Train: loss 2.2233, acc 0.2750 | Val: loss 2.2031, acc 0.3166\n",
      "Epoch   4/1000 | Train: loss 2.1926, acc 0.3330 | Val: loss 2.1810, acc 0.3414\n",
      "Epoch   5/1000 | Train: loss 2.1744, acc 0.3673 | Val: loss 2.1666, acc 0.3934\n",
      "Epoch   6/1000 | Train: loss 2.1603, acc 0.3904 | Val: loss 2.1553, acc 0.3952\n",
      "Epoch   7/1000 | Train: loss 2.1499, acc 0.4081 | Val: loss 2.1488, acc 0.4030\n",
      "Epoch   8/1000 | Train: loss 2.1413, acc 0.4190 | Val: loss 2.1426, acc 0.4240\n",
      "Epoch   9/1000 | Train: loss 2.1347, acc 0.4300 | Val: loss 2.1345, acc 0.4235\n",
      "Epoch  10/1000 | Train: loss 2.1267, acc 0.4405 | Val: loss 2.1303, acc 0.4272\n",
      "Epoch  11/1000 | Train: loss 2.1217, acc 0.4467 | Val: loss 2.1247, acc 0.4385\n",
      "Epoch  12/1000 | Train: loss 2.1153, acc 0.4555 | Val: loss 2.1203, acc 0.4418\n",
      "Epoch  13/1000 | Train: loss 2.1092, acc 0.4635 | Val: loss 2.1159, acc 0.4467\n",
      "Epoch  14/1000 | Train: loss 2.1036, acc 0.4717 | Val: loss 2.1102, acc 0.4550\n",
      "Epoch  15/1000 | Train: loss 2.0980, acc 0.4787 | Val: loss 2.1073, acc 0.4519\n",
      "Epoch  16/1000 | Train: loss 2.0941, acc 0.4832 | Val: loss 2.1058, acc 0.4622\n",
      "Epoch  17/1000 | Train: loss 2.0884, acc 0.4905 | Val: loss 2.1009, acc 0.4589\n",
      "Epoch  18/1000 | Train: loss 2.0829, acc 0.4974 | Val: loss 2.0965, acc 0.4702\n",
      "Epoch  19/1000 | Train: loss 2.0781, acc 0.5034 | Val: loss 2.0915, acc 0.4702\n",
      "Epoch  20/1000 | Train: loss 2.0728, acc 0.5103 | Val: loss 2.0896, acc 0.4797\n",
      "Epoch  21/1000 | Train: loss 2.0696, acc 0.5141 | Val: loss 2.0847, acc 0.4790\n",
      "Epoch  22/1000 | Train: loss 2.0635, acc 0.5222 | Val: loss 2.0817, acc 0.4778\n",
      "Epoch  23/1000 | Train: loss 2.0591, acc 0.5269 | Val: loss 2.0787, acc 0.4874\n",
      "Epoch  24/1000 | Train: loss 2.0553, acc 0.5301 | Val: loss 2.0755, acc 0.4925\n",
      "Epoch  25/1000 | Train: loss 2.0494, acc 0.5396 | Val: loss 2.0723, acc 0.4900\n",
      "Epoch  26/1000 | Train: loss 2.0446, acc 0.5457 | Val: loss 2.0692, acc 0.4893\n",
      "Epoch  27/1000 | Train: loss 2.0397, acc 0.5516 | Val: loss 2.0658, acc 0.4938\n",
      "Epoch  28/1000 | Train: loss 2.0364, acc 0.5554 | Val: loss 2.0646, acc 0.4976\n",
      "Epoch  29/1000 | Train: loss 2.0305, acc 0.5634 | Val: loss 2.0603, acc 0.4913\n",
      "Epoch  30/1000 | Train: loss 2.0288, acc 0.5620 | Val: loss 2.0575, acc 0.5006\n",
      "Epoch  31/1000 | Train: loss 2.0212, acc 0.5735 | Val: loss 2.0539, acc 0.5023\n",
      "Epoch  32/1000 | Train: loss 2.0172, acc 0.5792 | Val: loss 2.0515, acc 0.5080\n",
      "Epoch  33/1000 | Train: loss 2.0125, acc 0.5836 | Val: loss 2.0500, acc 0.5027\n",
      "Epoch  34/1000 | Train: loss 2.0091, acc 0.5846 | Val: loss 2.0466, acc 0.5057\n",
      "Epoch  35/1000 | Train: loss 2.0028, acc 0.5943 | Val: loss 2.0432, acc 0.5116\n",
      "Epoch  36/1000 | Train: loss 1.9985, acc 0.5999 | Val: loss 2.0391, acc 0.5105\n",
      "Epoch  37/1000 | Train: loss 1.9949, acc 0.6032 | Val: loss 2.0382, acc 0.5074\n"
     ]
    }
   ],
   "source": [
    "# Chebyshev FeedForward NN for CIFAR (Different Width for Different Layers)\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class SharedChebyshevLayer(nn.Module):\n",
    "    def __init__(self, degree: int, input_dim: int):\n",
    "        super().__init__()\n",
    "        self.degree = degree\n",
    "        self.coeffs = nn.Parameter(\n",
    "            torch.randn(input_dim, degree + 1, dtype=torch.float64)\n",
    "        )\n",
    "\n",
    "    def _chebyshev_basis(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Computes Chebyshev basis T_0 … T_degree for every scalar in x\n",
    "        without in-place ops (keeps autograd happy).\n",
    "        \"\"\"\n",
    "        bsz, feat = x.shape\n",
    "        x_flat = x.reshape(-1)\n",
    "\n",
    "        basis = [torch.ones_like(x_flat)]\n",
    "        if self.degree >= 1:\n",
    "            basis.append(x_flat)\n",
    "            for k in range(1, self.degree):\n",
    "                Tk = 2 * x_flat * basis[k] - basis[k - 1]\n",
    "                basis.append(Tk)\n",
    "\n",
    "        T = torch.stack(basis, dim=1)                   \n",
    "        return T.view(bsz, feat, self.degree + 1)      \n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        T = self._chebyshev_basis(x)\n",
    "        return torch.einsum(\"bik,ik->bi\", T, self.coeffs) \n",
    "\n",
    "\n",
    "class ChebyshevSharedNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_layers: int = 5,\n",
    "        cheb_degree: int = 15,\n",
    "        num_classes: int = 10,\n",
    "        hidden_dims: list[int] | None = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if hidden_dims is None:\n",
    "            hidden_dims = [3072] * (num_layers + 1)\n",
    "        assert len(hidden_dims) == num_layers + 1, (\n",
    "        )\n",
    "\n",
    "        self.init_linear   = nn.Linear(3072, hidden_dims[0], dtype=torch.float64)\n",
    "        self.transforms    = nn.ModuleList()\n",
    "        self.cheb_layers   = nn.ModuleList()\n",
    "\n",
    "        self.cheb_layers.append(SharedChebyshevLayer(cheb_degree, hidden_dims[0]))\n",
    "\n",
    "        for in_dim, out_dim in zip(hidden_dims[:-1], hidden_dims[1:]):\n",
    "            self.transforms.append(nn.Linear(in_dim, out_dim, dtype=torch.float64))\n",
    "            self.cheb_layers.append(SharedChebyshevLayer(cheb_degree, out_dim))\n",
    "\n",
    "        self.heads = nn.ModuleList(\n",
    "            [nn.Linear(hidden_dims[-1], 1, dtype=torch.float64) for _ in range(num_classes)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x.to(torch.float64)\n",
    "\n",
    "        # Initial projection + Chebyshev modulation\n",
    "        x = F.normalize(self.init_linear(x), p=2, dim=1)\n",
    "        x = F.normalize(self.cheb_layers[0](x), p=2, dim=1)\n",
    "\n",
    "        # Main stack\n",
    "        for linear, cheb in zip(self.transforms, self.cheb_layers[1:]):\n",
    "            x = F.normalize(linear(x),           p=2, dim=1)\n",
    "            x = F.normalize(cheb(x),             p=2, dim=1)\n",
    "        return torch.cat([head(x) for head in self.heads], dim=1)\n",
    "\n",
    "input_vectors = torch.tensor(train_images, dtype=torch.float64)\n",
    "target_labels = torch.tensor(train_labels, dtype=torch.long)\n",
    "\n",
    "train_ds = TensorDataset(input_vectors, target_labels)\n",
    "test_ds  = TensorDataset(\n",
    "    torch.tensor(test_images, dtype=torch.float64),\n",
    "    torch.tensor(test_labels, dtype=torch.long)\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=1000, shuffle=True)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=1000, shuffle=False)\n",
    "\n",
    "model = ChebyshevSharedNN(\n",
    "    num_layers   = 5,\n",
    "    cheb_degree  = 20,\n",
    "    hidden_dims  = [3072, 1536 , 768 , 384 , 192 , 81 ], \n",
    ").to(device)\n",
    "\n",
    "criterion  = nn.CrossEntropyLoss()\n",
    "optimizer  = optim.Adam(model.parameters(), lr=1e-5, weight_decay=1e-4)\n",
    "scheduler  = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=2, factor=0.5)\n",
    "\n",
    "EPOCHS = 1000\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    model.train()\n",
    "    running_loss, running_correct = 0.0, 0\n",
    "    for X, y in train_loader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits = model(X)\n",
    "        loss   = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss   += loss.item() * X.size(0)\n",
    "        running_correct += (logits.argmax(1) == y).sum().item()\n",
    "\n",
    "    train_loss = running_loss / len(train_loader.dataset)\n",
    "    train_acc  = running_correct / len(train_loader.dataset)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss, val_correct = 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in test_loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            logits = model(X)\n",
    "\n",
    "            val_loss     += criterion(logits, y).item() * X.size(0)\n",
    "            val_correct  += (logits.argmax(1) == y).sum().item()\n",
    "\n",
    "    val_loss /= len(test_loader.dataset)\n",
    "    val_acc  = val_correct / len(test_loader.dataset)\n",
    "\n",
    "    scheduler.step(val_loss)\n",
    "    print(\n",
    "        f\"Epoch {epoch:3d}/{EPOCHS} | \"\n",
    "        f\"Train: loss {train_loss:.4f}, acc {train_acc:.4f} | \"\n",
    "        f\"Val: loss {val_loss:.4f}, acc {val_acc:.4f}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9f2e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Akshay Patil\\AppData\\Local\\Temp\\ipykernel_19272\\2432826012.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_vectors = torch.tensor(train_images, dtype=torch.float64)\n",
      "C:\\Users\\Akshay Patil\\AppData\\Local\\Temp\\ipykernel_19272\\2432826012.py:113: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(test_images, dtype=torch.float64),\n",
      "C:\\Users\\Akshay Patil\\AppData\\Local\\Temp\\ipykernel_19272\\2432826012.py:114: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(test_labels, dtype=torch.long)\n"
     ]
    }
   ],
   "source": [
    "# Direct Lagrangian OVR (Per Class , Different Strategies can be used as in Lagrangian NN paper , also depth and width can be changed based on traditional\n",
    "# NN methods that gives the best result accuracy wise) - \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "input_vectors = torch.tensor(train_images, dtype=torch.float64)\n",
    "target_labels = train_labels  # Convert one-hot to class labels\n",
    "\n",
    "class LagrangianNNOVRClassifier(nn.Module):\n",
    "    def __init__(self, degree, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.degree = degree\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        self.transformation_params = nn.ParameterList([\n",
    "            nn.Parameter(torch.empty(3072,3072 , dtype=torch.float64)) \n",
    "            for _ in range(degree)\n",
    "        ])\n",
    "        \n",
    "\n",
    "        self.bias_params = nn.ParameterList([\n",
    "            nn.Parameter(torch.empty(3072, dtype=torch.float64))\n",
    "            for _ in range(degree)\n",
    "        ])\n",
    "        \n",
    "\n",
    "        self.alpha_params = nn.ParameterList([\n",
    "            nn.Parameter(torch.empty(num_classes, 3072, dtype=torch.float64))\n",
    "            for _ in range(degree-1)\n",
    "        ])\n",
    "        \n",
    "\n",
    "        self.class_projections = nn.ModuleList([\n",
    "            nn.Linear(3072, 1, dtype=torch.float64) \n",
    "            for _ in range(num_classes)\n",
    "        ])\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for param in self.transformation_params:\n",
    "                nn.init.kaiming_normal_(param, mode='fan_in', nonlinearity='linear')\n",
    "            for bias in self.bias_params:\n",
    "                nn.init.zeros_(bias)  \n",
    "            for alpha in self.alpha_params:\n",
    "                nn.init.normal_(alpha)\n",
    "            for proj in self.class_projections:\n",
    "                nn.init.kaiming_normal_(proj.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(torch.float64)\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "\n",
    "        x = torch.matmul(self.transformation_params[0], x.unsqueeze(-1)).squeeze()\n",
    "        x = x + self.bias_params[0]  \n",
    "        \n",
    "\n",
    "        class_outputs = []\n",
    "        for c in range(self.num_classes):\n",
    "            x_class = x.clone()\n",
    "            \n",
    "\n",
    "            for i, (transform, bias, alpha) in enumerate(zip(\n",
    "                self.transformation_params[1:], \n",
    "                self.bias_params[1:], \n",
    "                self.alpha_params\n",
    "            )):\n",
    "\n",
    "                alpha_clamped = torch.sigmoid(alpha[c])\n",
    "                modulated = x_class * alpha_clamped\n",
    "                \n",
    "                modulated = torch.matmul(transform, modulated.unsqueeze(-1)).squeeze()\n",
    "                modulated = modulated + bias  \n",
    "                x_class = F.normalize(modulated, p=2.0, dim=1)\n",
    "            \n",
    "            class_outputs.append(x_class)\n",
    "        \n",
    "        scores = torch.cat([\n",
    "            proj(features) for proj, features in zip(self.class_projections, class_outputs)\n",
    "        ], dim=1)\n",
    "        \n",
    "        return scores\n",
    "\n",
    "def ovr_hinge_loss(scores, targets):\n",
    "    batch_size = scores.size(0)\n",
    "    correct_scores = scores[torch.arange(batch_size), targets].unsqueeze(1)\n",
    "    margins = 1 - (correct_scores - scores)\n",
    "    margins[torch.arange(batch_size), targets] = 0 \n",
    "    return torch.clamp(margins, min=0).sum(dim=1).mean()\n",
    "\n",
    "degree = 10\n",
    "num_classes = 10\n",
    "model = LagrangianNNOVRClassifier(degree, num_classes).to(device)\n",
    "\n",
    "dataset = TensorDataset(input_vectors, train_labels)\n",
    "batch_size = 512\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(\n",
    "    torch.tensor(test_images, dtype=torch.float64),\n",
    "    torch.tensor(test_labels, dtype=torch.long)\n",
    ")\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-2, weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=2, factor=0.5)\n",
    "\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    train_correct = 0\n",
    "    \n",
    "    for batch_input, batch_target in dataloader:\n",
    "        batch_input, batch_target = batch_input.to(device), batch_target.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        scores = model(batch_input)\n",
    "        loss = ovr_hinge_loss(scores, batch_target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        preds = torch.argmax(scores, dim=1)\n",
    "        train_correct += (preds == batch_target).sum().item()\n",
    "    \n",
    "    model.eval()\n",
    "    val_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_input, batch_target in dataloader:\n",
    "            batch_input, batch_target = batch_input.to(device), batch_target.to(device)\n",
    "            scores = model(batch_input)\n",
    "            preds = torch.argmax(scores, dim=1)\n",
    "            val_correct += (preds == batch_target).sum().item()\n",
    "\n",
    "    test_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_input, batch_target in test_dataloader:\n",
    "            batch_input, batch_target = batch_input.to(device), batch_target.to(device)\n",
    "            scores = model(batch_input)\n",
    "            preds = torch.argmax(scores, dim=1)\n",
    "            test_correct += (preds == batch_target).sum().item()\n",
    "    \n",
    "    \n",
    "    train_acc = train_correct / len(dataset)\n",
    "    val_acc = val_correct / len(dataset)\n",
    "    test_acc = test_correct / len(test_dataset)\n",
    "    avg_loss = epoch_loss / len(dataloader)\n",
    "    \n",
    "    scheduler.step(avg_loss)\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/100] |'\n",
    "          f'Loss: {avg_loss:.4f} |'\n",
    "          f'Train Acc: {train_acc:.4f} |'\n",
    "          f'Val Acc: {val_acc:.4f} |'\n",
    "          f'Test Acc: {test_acc:.4f}|')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b1b9b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized with 10000 parameters\n",
      "Training on 49999 samples, testing on 10000 samples\n",
      "Epoch 1, Batch 0, Loss: 8.9698\n",
      "Epoch 1, Batch 100, Loss: 4.2937\n",
      "Epoch [1/100] | Loss: 5.3860 | Train Acc: 0.2946 | Val Acc: 0.3659 | Test Acc: 0.3674 | Best Test: 0.3674\n",
      "Epoch 2, Batch 0, Loss: 3.9823\n",
      "Epoch 2, Batch 100, Loss: 3.6648\n",
      "Epoch [2/100] | Loss: 3.8197 | Train Acc: 0.4019 | Val Acc: 0.3992 | Test Acc: 0.4001 | Best Test: 0.4001\n",
      "Epoch 3, Batch 0, Loss: 3.6025\n",
      "Epoch 3, Batch 100, Loss: 3.5653\n",
      "Epoch [3/100] | Loss: 3.3597 | Train Acc: 0.4425 | Val Acc: 0.4568 | Test Acc: 0.4483 | Best Test: 0.4483\n",
      "Epoch 4, Batch 0, Loss: 3.4796\n",
      "Epoch 4, Batch 100, Loss: 2.7732\n",
      "Epoch [4/100] | Loss: 3.0767 | Train Acc: 0.4677 | Val Acc: 0.4801 | Test Acc: 0.4726 | Best Test: 0.4726\n",
      "Epoch 5, Batch 0, Loss: 3.1537\n",
      "Epoch 5, Batch 100, Loss: 3.1095\n",
      "Epoch [5/100] | Loss: 2.8908 | Train Acc: 0.4888 | Val Acc: 0.4802 | Test Acc: 0.4741 | Best Test: 0.4741\n",
      "Epoch 6, Batch 0, Loss: 3.0613\n",
      "Epoch 6, Batch 100, Loss: 2.6517\n",
      "Epoch [6/100] | Loss: 2.7485 | Train Acc: 0.5017 | Val Acc: 0.4959 | Test Acc: 0.4811 | Best Test: 0.4811\n",
      "Epoch 7, Batch 0, Loss: 2.3330\n",
      "Epoch 7, Batch 100, Loss: 2.6240\n",
      "Epoch [7/100] | Loss: 2.6464 | Train Acc: 0.5132 | Val Acc: 0.4995 | Test Acc: 0.4892 | Best Test: 0.4892\n",
      "Epoch 8, Batch 0, Loss: 2.8581\n",
      "Epoch 8, Batch 100, Loss: 2.5344\n",
      "Epoch [8/100] | Loss: 2.5586 | Train Acc: 0.5221 | Val Acc: 0.5026 | Test Acc: 0.4888 | Best Test: 0.4892\n",
      "Epoch 9, Batch 0, Loss: 2.3643\n",
      "Epoch 9, Batch 100, Loss: 2.3214\n",
      "Epoch [9/100] | Loss: 2.4704 | Train Acc: 0.5340 | Val Acc: 0.5089 | Test Acc: 0.4937 | Best Test: 0.4937\n",
      "Epoch 10, Batch 0, Loss: 2.0777\n",
      "Epoch 10, Batch 100, Loss: 2.3174\n",
      "Epoch [10/100] | Loss: 2.4095 | Train Acc: 0.5434 | Val Acc: 0.5215 | Test Acc: 0.5006 | Best Test: 0.5006\n",
      "Epoch 11, Batch 0, Loss: 2.2310\n",
      "Epoch 11, Batch 100, Loss: 2.1497\n",
      "Epoch [11/100] | Loss: 2.3405 | Train Acc: 0.5536 | Val Acc: 0.5277 | Test Acc: 0.5110 | Best Test: 0.5110\n",
      "Epoch 12, Batch 0, Loss: 2.3157\n",
      "Epoch 12, Batch 100, Loss: 2.5992\n",
      "Epoch [12/100] | Loss: 2.2726 | Train Acc: 0.5616 | Val Acc: 0.5259 | Test Acc: 0.5100 | Best Test: 0.5110\n",
      "Epoch 13, Batch 0, Loss: 2.1361\n",
      "Epoch 13, Batch 100, Loss: 2.2108\n",
      "Epoch [13/100] | Loss: 2.2312 | Train Acc: 0.5671 | Val Acc: 0.5413 | Test Acc: 0.5229 | Best Test: 0.5229\n",
      "Epoch 14, Batch 0, Loss: 2.0985\n",
      "Epoch 14, Batch 100, Loss: 2.4242\n",
      "Epoch [14/100] | Loss: 2.1953 | Train Acc: 0.5740 | Val Acc: 0.5296 | Test Acc: 0.5129 | Best Test: 0.5229\n",
      "Epoch 15, Batch 0, Loss: 2.1474\n",
      "Epoch 15, Batch 100, Loss: 2.2588\n",
      "Epoch [15/100] | Loss: 2.1481 | Train Acc: 0.5787 | Val Acc: 0.5197 | Test Acc: 0.5116 | Best Test: 0.5229\n",
      "Epoch 16, Batch 0, Loss: 2.0521\n",
      "Epoch 16, Batch 100, Loss: 2.3251\n",
      "Epoch [16/100] | Loss: 2.1129 | Train Acc: 0.5849 | Val Acc: 0.5500 | Test Acc: 0.5327 | Best Test: 0.5327\n",
      "Epoch 17, Batch 0, Loss: 2.3656\n",
      "Epoch 17, Batch 100, Loss: 1.6785\n",
      "Epoch [17/100] | Loss: 2.0746 | Train Acc: 0.5904 | Val Acc: 0.5279 | Test Acc: 0.5094 | Best Test: 0.5327\n",
      "Epoch 18, Batch 0, Loss: 2.0419\n",
      "Epoch 18, Batch 100, Loss: 2.1623\n",
      "Epoch [18/100] | Loss: 2.0465 | Train Acc: 0.5941 | Val Acc: 0.5425 | Test Acc: 0.5266 | Best Test: 0.5327\n",
      "Epoch 19, Batch 0, Loss: 2.1292\n",
      "Epoch 19, Batch 100, Loss: 2.0032\n",
      "Epoch [19/100] | Loss: 2.0125 | Train Acc: 0.5993 | Val Acc: 0.5543 | Test Acc: 0.5353 | Best Test: 0.5353\n",
      "Epoch 20, Batch 0, Loss: 2.0195\n",
      "Epoch 20, Batch 100, Loss: 1.6946\n",
      "Epoch [20/100] | Loss: 1.9936 | Train Acc: 0.6028 | Val Acc: 0.5296 | Test Acc: 0.5126 | Best Test: 0.5353\n",
      "Epoch 21, Batch 0, Loss: 1.5559\n",
      "Epoch 21, Batch 100, Loss: 1.7682\n",
      "Epoch [21/100] | Loss: 1.9657 | Train Acc: 0.6069 | Val Acc: 0.5314 | Test Acc: 0.5113 | Best Test: 0.5353\n",
      "Epoch 22, Batch 0, Loss: 1.4803\n",
      "Epoch 22, Batch 100, Loss: 1.6041\n",
      "Epoch [22/100] | Loss: 1.9334 | Train Acc: 0.6102 | Val Acc: 0.5570 | Test Acc: 0.5402 | Best Test: 0.5402\n",
      "Epoch 23, Batch 0, Loss: 1.8372\n",
      "Epoch 23, Batch 100, Loss: 2.1235\n",
      "Epoch [23/100] | Loss: 1.9232 | Train Acc: 0.6138 | Val Acc: 0.5725 | Test Acc: 0.5556 | Best Test: 0.5556\n",
      "Epoch 24, Batch 0, Loss: 1.8735\n",
      "Epoch 24, Batch 100, Loss: 1.7944\n",
      "Epoch [24/100] | Loss: 1.9015 | Train Acc: 0.6156 | Val Acc: 0.5683 | Test Acc: 0.5487 | Best Test: 0.5556\n",
      "Epoch 25, Batch 0, Loss: 1.5461\n",
      "Epoch 25, Batch 100, Loss: 1.8640\n",
      "Epoch [25/100] | Loss: 1.8841 | Train Acc: 0.6188 | Val Acc: 0.5619 | Test Acc: 0.5469 | Best Test: 0.5556\n",
      "Epoch 26, Batch 0, Loss: 1.9805\n",
      "Epoch 26, Batch 100, Loss: 1.6977\n",
      "Epoch [26/100] | Loss: 1.8691 | Train Acc: 0.6203 | Val Acc: 0.5587 | Test Acc: 0.5357 | Best Test: 0.5556\n",
      "Epoch 27, Batch 0, Loss: 1.6677\n",
      "Epoch 27, Batch 100, Loss: 1.8761\n",
      "Epoch [27/100] | Loss: 1.8491 | Train Acc: 0.6244 | Val Acc: 0.5607 | Test Acc: 0.5389 | Best Test: 0.5556\n",
      "Epoch 28, Batch 0, Loss: 1.6148\n",
      "Epoch 28, Batch 100, Loss: 1.9033\n",
      "Epoch [28/100] | Loss: 1.8294 | Train Acc: 0.6258 | Val Acc: 0.5592 | Test Acc: 0.5425 | Best Test: 0.5556\n",
      "Epoch 29, Batch 0, Loss: 1.7870\n",
      "Epoch 29, Batch 100, Loss: 2.1262\n",
      "Epoch [29/100] | Loss: 1.8121 | Train Acc: 0.6293 | Val Acc: 0.5550 | Test Acc: 0.5341 | Best Test: 0.5556\n",
      "Epoch 30, Batch 0, Loss: 2.0257\n",
      "Epoch 30, Batch 100, Loss: 1.6737\n",
      "Epoch [30/100] | Loss: 1.7944 | Train Acc: 0.6323 | Val Acc: 0.4975 | Test Acc: 0.4776 | Best Test: 0.5556\n",
      "Epoch 31, Batch 0, Loss: 1.4734\n",
      "Epoch 31, Batch 100, Loss: 2.0182\n",
      "Epoch [31/100] | Loss: 1.7803 | Train Acc: 0.6345 | Val Acc: 0.5538 | Test Acc: 0.5327 | Best Test: 0.5556\n",
      "Epoch 32, Batch 0, Loss: 1.8072\n",
      "Epoch 32, Batch 100, Loss: 1.8390\n",
      "Epoch [32/100] | Loss: 1.7784 | Train Acc: 0.6347 | Val Acc: 0.5619 | Test Acc: 0.5405 | Best Test: 0.5556\n",
      "Epoch 33, Batch 0, Loss: 1.8568\n",
      "Epoch 33, Batch 100, Loss: 1.8541\n",
      "Epoch [33/100] | Loss: 1.7648 | Train Acc: 0.6366 | Val Acc: 0.5727 | Test Acc: 0.5525 | Best Test: 0.5556\n",
      "Epoch 34, Batch 0, Loss: 1.6402\n",
      "Epoch 34, Batch 100, Loss: 1.8344\n",
      "Epoch [34/100] | Loss: 1.7506 | Train Acc: 0.6396 | Val Acc: 0.5817 | Test Acc: 0.5596 | Best Test: 0.5596\n",
      "Epoch 35, Batch 0, Loss: 1.2957\n",
      "Epoch 35, Batch 100, Loss: 1.7763\n",
      "Epoch [35/100] | Loss: 1.7315 | Train Acc: 0.6411 | Val Acc: 0.5744 | Test Acc: 0.5543 | Best Test: 0.5596\n",
      "Epoch 36, Batch 0, Loss: 1.5840\n",
      "Epoch 36, Batch 100, Loss: 1.5996\n",
      "Epoch [36/100] | Loss: 1.7348 | Train Acc: 0.6392 | Val Acc: 0.5516 | Test Acc: 0.5321 | Best Test: 0.5596\n",
      "Epoch 37, Batch 0, Loss: 1.7160\n",
      "Epoch 37, Batch 100, Loss: 1.8580\n",
      "Epoch [37/100] | Loss: 1.7182 | Train Acc: 0.6414 | Val Acc: 0.5627 | Test Acc: 0.5356 | Best Test: 0.5596\n",
      "Epoch 38, Batch 0, Loss: 1.6348\n",
      "Epoch 38, Batch 100, Loss: 1.5057\n",
      "Epoch [38/100] | Loss: 1.7034 | Train Acc: 0.6452 | Val Acc: 0.5865 | Test Acc: 0.5599 | Best Test: 0.5599\n",
      "Epoch 39, Batch 0, Loss: 1.5437\n",
      "Epoch 39, Batch 100, Loss: 1.8142\n",
      "Epoch [39/100] | Loss: 1.6916 | Train Acc: 0.6463 | Val Acc: 0.5577 | Test Acc: 0.5317 | Best Test: 0.5599\n",
      "Epoch 40, Batch 0, Loss: 1.6676\n",
      "Epoch 40, Batch 100, Loss: 1.5859\n",
      "Epoch [40/100] | Loss: 1.6854 | Train Acc: 0.6455 | Val Acc: 0.5700 | Test Acc: 0.5444 | Best Test: 0.5599\n",
      "Epoch 41, Batch 0, Loss: 1.6994\n",
      "Epoch 41, Batch 100, Loss: 1.8034\n",
      "Epoch [41/100] | Loss: 1.6754 | Train Acc: 0.6469 | Val Acc: 0.5628 | Test Acc: 0.5466 | Best Test: 0.5599\n",
      "Epoch 42, Batch 0, Loss: 1.6246\n",
      "Epoch 42, Batch 100, Loss: 1.9592\n",
      "Epoch [42/100] | Loss: 1.6690 | Train Acc: 0.6490 | Val Acc: 0.5497 | Test Acc: 0.5279 | Best Test: 0.5599\n",
      "Epoch 43, Batch 0, Loss: 1.6594\n",
      "Epoch 43, Batch 100, Loss: 1.5651\n",
      "Epoch [43/100] | Loss: 1.6649 | Train Acc: 0.6498 | Val Acc: 0.5545 | Test Acc: 0.5367 | Best Test: 0.5599\n",
      "Epoch 44, Batch 0, Loss: 1.8296\n",
      "Epoch 44, Batch 100, Loss: 1.3888\n",
      "Epoch [44/100] | Loss: 1.6428 | Train Acc: 0.6506 | Val Acc: 0.5366 | Test Acc: 0.5151 | Best Test: 0.5599\n",
      "Epoch 45, Batch 0, Loss: 1.3868\n",
      "Epoch 45, Batch 100, Loss: 1.6602\n",
      "Epoch [45/100] | Loss: 1.6444 | Train Acc: 0.6513 | Val Acc: 0.5719 | Test Acc: 0.5478 | Best Test: 0.5599\n",
      "Epoch 46, Batch 0, Loss: 1.5679\n",
      "Epoch 46, Batch 100, Loss: 1.7353\n",
      "Epoch [46/100] | Loss: 1.6323 | Train Acc: 0.6551 | Val Acc: 0.5641 | Test Acc: 0.5409 | Best Test: 0.5599\n",
      "Epoch 47, Batch 0, Loss: 1.4933\n",
      "Epoch 47, Batch 100, Loss: 1.1345\n",
      "Epoch [47/100] | Loss: 1.6249 | Train Acc: 0.6533 | Val Acc: 0.5645 | Test Acc: 0.5440 | Best Test: 0.5599\n",
      "Epoch 48, Batch 0, Loss: 1.3607\n",
      "Epoch 48, Batch 100, Loss: 1.3026\n"
     ]
    }
   ],
   "source": [
    "# Convolutional Lagrangian OVR (per class and not per sample) - Just 10000 Parameters , Optimum Arcitechtures that work best for this via traditional NN that \n",
    "# are already implemented can be used.\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def generate_sample_data():\n",
    "    return train_images.reshape(-1,3,32,32), train_labels, test_images.reshape(-1,3,32,32), test_labels\n",
    "\n",
    "class ConvolutionalLagrangianNN(nn.Module):\n",
    "    \"\"\"\n",
    "    10 shared Conv layers + class-specific alpha modulations +\n",
    "    1 Lagrangian transform layer + ten 1-D heads.\n",
    "\n",
    "    Total parameters: 10 000\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 num_classes: int = 10,\n",
    "                 conv_degree: int = 10,\n",
    "                 lagrangian_degree: int = 1):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.conv_degree = conv_degree\n",
    "        self.lagrangian_degree = lagrangian_degree\n",
    "\n",
    "\n",
    "        channels = [3] + [9] * conv_degree          \n",
    "        self.pooling_layers = {1, 3, 5, 7, 9}     \n",
    "\n",
    "        self.conv_layers      = nn.ModuleList()\n",
    "        self.conv_batch_norms = nn.ModuleList()\n",
    "        for i in range(conv_degree):\n",
    "            self.conv_layers.append(\n",
    "                nn.Conv2d(channels[i], channels[i + 1],\n",
    "                          kernel_size=3, padding=1, bias=True)\n",
    "            )\n",
    "            self.conv_batch_norms.append(\n",
    "                nn.BatchNorm2d(channels[i + 1])\n",
    "            )\n",
    "\n",
    "        self.final_conv_size  = 1 * 1 * channels[-1]    \n",
    "        self.lagrangian_size  = 32                     \n",
    "        self.dim_expansion    = nn.Linear(self.final_conv_size,\n",
    "                                          self.lagrangian_size)\n",
    "\n",
    "        self.conv_alpha_params = nn.ParameterList([\n",
    "            nn.Parameter(torch.empty(num_classes, channels[i + 1]))\n",
    "            for i in range(conv_degree)\n",
    "        ])\n",
    "\n",
    "        self.lagrangian_transforms = nn.ParameterList([\n",
    "            nn.Parameter(torch.empty(self.lagrangian_size,\n",
    "                                     self.lagrangian_size))\n",
    "            for _ in range(lagrangian_degree)\n",
    "        ])\n",
    "        self.lagrangian_biases = nn.ParameterList([\n",
    "            nn.Parameter(torch.empty(self.lagrangian_size))\n",
    "            for _ in range(lagrangian_degree)\n",
    "        ])\n",
    "        self.lagrangian_alpha_params = nn.ParameterList([\n",
    "            nn.Parameter(torch.empty(num_classes, self.lagrangian_size))\n",
    "            for _ in range(lagrangian_degree)\n",
    "        ])\n",
    "\n",
    "        self.class_projections = nn.ModuleList([\n",
    "            nn.Linear(self.lagrangian_size, 1)\n",
    "            for _ in range(num_classes)\n",
    "        ])\n",
    "\n",
    "        self._initialize_parameters()\n",
    "\n",
    "\n",
    "    def _initialize_parameters(self):\n",
    "        with torch.no_grad():\n",
    "            for conv in self.conv_layers:\n",
    "                nn.init.kaiming_normal_(conv.weight, mode='fan_out',\n",
    "                                        nonlinearity='relu')\n",
    "                nn.init.zeros_(conv.bias)\n",
    "            nn.init.kaiming_normal_(self.dim_expansion.weight,\n",
    "                                    mode='fan_in', nonlinearity='linear')\n",
    "            nn.init.zeros_(self.dim_expansion.bias)\n",
    "\n",
    "            for alpha in self.conv_alpha_params:\n",
    "                nn.init.normal_(alpha, mean=0., std=0.1)\n",
    "            for T in self.lagrangian_transforms:\n",
    "                nn.init.kaiming_normal_(T, mode='fan_in',\n",
    "                                        nonlinearity='linear')\n",
    "            for b in self.lagrangian_biases:\n",
    "                nn.init.zeros_(b)\n",
    "            for alpha in self.lagrangian_alpha_params:\n",
    "                nn.init.normal_(alpha, mean=0., std=0.1)\n",
    "            for proj in self.class_projections:\n",
    "                nn.init.kaiming_normal_(proj.weight)\n",
    "                nn.init.zeros_(proj.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.float()\n",
    "        batch_size = x.size(0)\n",
    "        class_outputs = []\n",
    "\n",
    "        for c in range(self.num_classes):\n",
    "            x_c = x.clone()\n",
    "\n",
    "            for i, (conv, bn, alpha) in enumerate(zip(\n",
    "                    self.conv_layers, self.conv_batch_norms,\n",
    "                    self.conv_alpha_params)):\n",
    "                x_c = conv(x_c)\n",
    "                x_c = bn(x_c)\n",
    "                alpha_c = torch.sigmoid(alpha[c]).view(1, -1, 1, 1)\n",
    "                x_c = x_c * alpha_c\n",
    "\n",
    "                if (i + 1) in self.pooling_layers:\n",
    "                    x_c = F.max_pool2d(x_c, 2)\n",
    "\n",
    "            x_c = self.dim_expansion(x_c.view(batch_size, -1))\n",
    "\n",
    "            T, b = self.lagrangian_transforms[0], self.lagrangian_biases[0]\n",
    "            x_c = x_c @ T.t() + b\n",
    "            alpha_L = torch.sigmoid(self.lagrangian_alpha_params[0][c])\n",
    "            x_c = x_c * alpha_L\n",
    "            x_c = F.normalize(x_c, p=2.0, dim=1)\n",
    "\n",
    "            class_outputs.append(x_c)\n",
    "\n",
    "        scores = torch.cat(\n",
    "            [proj(feat) for proj, feat in zip(self.class_projections,\n",
    "                                              class_outputs)],\n",
    "            dim=1)\n",
    "        return scores\n",
    "\n",
    "\n",
    "def ovr_hinge_loss(scores, targets):\n",
    "    \"\"\"One-vs-rest hinge loss (margin = 1).\"\"\"\n",
    "    correct = scores[torch.arange(scores.size(0)), targets].unsqueeze(1)\n",
    "    margins = 1 - (correct - scores)\n",
    "    margins[torch.arange(scores.size(0)), targets] = 0\n",
    "    return torch.clamp(margins, min=0).sum(dim=1).mean()\n",
    "\n",
    "\n",
    "def train_model():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    input_images, class_labels, test_images, test_labels = generate_sample_data()\n",
    "\n",
    "    \n",
    "    model = ConvolutionalLagrangianNN().to(device)\n",
    "\n",
    "    train_dataset = TensorDataset(input_images, class_labels)\n",
    "    test_dataset = TensorDataset(test_images, test_labels)\n",
    "    \n",
    "    batch_size = 256\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, factor=0.5)\n",
    "    \n",
    "    print(f\"Model initialized with {sum(p.numel() for p in model.parameters())} parameters\")\n",
    "    print(f\"Training on {len(train_dataset)} samples, testing on {len(test_dataset)} samples\")\n",
    "\n",
    "    num_epochs = 100\n",
    "    best_test_acc = 0.0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        train_correct = 0\n",
    "        \n",
    "        for batch_idx, (batch_input, batch_target) in enumerate(train_dataloader):\n",
    "            batch_input, batch_target = batch_input.to(device), batch_target.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            scores = model(batch_input)\n",
    "            loss = ovr_hinge_loss(scores, batch_target)\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            preds = torch.argmax(scores, dim=1)\n",
    "            train_correct += (preds == batch_target).sum().item()\n",
    "            \n",
    "            if batch_idx % 100 == 0:\n",
    "                print(f'Epoch {epoch+1}, Batch {batch_idx}, Loss: {loss.item():.4f}')\n",
    "        \n",
    "        if epoch % 20 == 0:\n",
    "            model.eval()\n",
    "            val_correct = 0\n",
    "            with torch.no_grad():\n",
    "                for batch_input, batch_target in train_dataloader:\n",
    "                    batch_input, batch_target = batch_input.to(device), batch_target.to(device)\n",
    "                    scores = model(batch_input)\n",
    "                    preds = torch.argmax(scores, dim=1)\n",
    "                    val_correct += (preds == batch_target).sum().item()\n",
    "\n",
    "            test_correct = 0\n",
    "            with torch.no_grad():\n",
    "                for batch_input, batch_target in test_dataloader:\n",
    "                    batch_input, batch_target = batch_input.to(device), batch_target.to(device)\n",
    "                    scores = model(batch_input)\n",
    "                    preds = torch.argmax(scores, dim=1)\n",
    "                    test_correct += (preds == batch_target).sum().item()\n",
    "    \n",
    "            train_acc = train_correct / len(train_dataset)\n",
    "            val_acc = val_correct / len(train_dataset)\n",
    "            test_acc = test_correct / len(test_dataset)\n",
    "\n",
    "        avg_loss = epoch_loss / len(train_dataloader)\n",
    "\n",
    "        scheduler.step(avg_loss)\n",
    "\n",
    "        if test_acc > best_test_acc:\n",
    "            best_test_acc = test_acc\n",
    "            torch.save(model.state_dict(), 'best_conv_lagrangian_model.pth')\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}] | '\n",
    "              f'Loss: {avg_loss:.4f} | '\n",
    "              f'Train Acc: {train_acc:.4f} | '\n",
    "              f'Val Acc: {val_acc:.4f} | '\n",
    "              f'Test Acc: {test_acc:.4f} | '\n",
    "              f'Best Test: {best_test_acc:.4f}')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_model()\n",
    "\n",
    "# One can implement all the Methods or Strategies discussed in the Best NN is Lagrangian NN Paper..here only OVR along with convolution is implemented...\n",
    "# One can also increase the number of parameters as here only 10000 parameters are considered....\n",
    "# And also evaluation can be done only at particular epochs to save the compute time drastically..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af4af80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Any and Every Existing type of NN architechture can be applied the Lagrangian Per Sample (Probabilistic Interpretation of NN from Getting Exact Equations of NN\n",
    "# Paper), Lagrangian Per Class (SVM Analog of NN - Various Stategies from Best NN is Lagrangian NN Paper) or Chebyshev Learnable Activation (Deterministic Interpretation  \n",
    "# of NN Paper along with Bayes Theorem Every Term Explicity Computed for NN paper) - Advantage of using this is fully Interpretable Equation wise version \n",
    "# of NN (either N linear Lagrangian Equation for N samples , or M linear Lagrangian Equations (Various Stragtegies) for M classes of N samples or one\n",
    "# single high degree Polynomial equation for all classes (Chebyshev Activation NN))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e30a425",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7ce5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other Strategies in the Lagrangian OVR other than the one mentioned in paper is , forming groups per class and assigning one lagrangian vector to that group\n",
    "# to improve the expressivity , if the groups are equal to 1 then it forms as Lagrangian per sample , a case which is already implemented..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1b91f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params: 112,556\n",
      "Batch Number 0\n",
      "Batch Number 100\n",
      "Batch Number 200\n",
      "Batch Number 300\n",
      "Batch Number 400\n",
      "Batch Number 500\n",
      "Batch Number 600\n",
      "Batch Number 700\n",
      "Ep 01 | Train 1.778/0.348 | Val 1.543/0.446 | Train Accuracy 0.348/0.348 | Test Accuracy 0.446/0.446 | LR 1.00e-03 | Time 28.2m\n",
      "Batch Number 0\n",
      "Batch Number 100\n",
      "Batch Number 200\n",
      "Batch Number 300\n",
      "Batch Number 400\n",
      "Batch Number 500\n",
      "Batch Number 600\n",
      "Batch Number 700\n",
      "Ep 02 | Train 1.436/0.482 | Val 1.399/0.493 | Train Accuracy 0.482/0.482 | Test Accuracy 0.493/0.493 | LR 1.00e-03 | Time 57.0m\n",
      "Batch Number 0\n",
      "Batch Number 100\n",
      "Batch Number 200\n",
      "Batch Number 300\n",
      "Batch Number 400\n",
      "Batch Number 500\n",
      "Batch Number 600\n",
      "Batch Number 700\n",
      "Ep 03 | Train 1.257/0.551 | Val 1.215/0.564 | Train Accuracy 0.551/0.551 | Test Accuracy 0.564/0.564 | LR 1.00e-03 | Time 86.3m\n",
      "Batch Number 0\n",
      "Batch Number 100\n",
      "Batch Number 200\n",
      "Batch Number 300\n",
      "Batch Number 400\n",
      "Batch Number 500\n",
      "Batch Number 600\n",
      "Batch Number 700\n",
      "Ep 04 | Train 1.138/0.594 | Val 1.139/0.593 | Train Accuracy 0.594/0.594 | Test Accuracy 0.593/0.593 | LR 1.00e-03 | Time 114.7m\n",
      "Batch Number 0\n",
      "Batch Number 100\n",
      "Batch Number 200\n",
      "Batch Number 300\n",
      "Batch Number 400\n",
      "Batch Number 500\n",
      "Batch Number 600\n",
      "Batch Number 700\n",
      "Ep 05 | Train 1.057/0.625 | Val 1.096/0.615 | Train Accuracy 0.625/0.625 | Test Accuracy 0.615/0.615 | LR 1.00e-03 | Time 143.0m\n",
      "Batch Number 0\n",
      "Batch Number 100\n",
      "Batch Number 200\n",
      "Batch Number 300\n",
      "Batch Number 400\n",
      "Batch Number 500\n",
      "Batch Number 600\n",
      "Batch Number 700\n",
      "Ep 06 | Train 0.991/0.649 | Val 1.037/0.627 | Train Accuracy 0.649/0.649 | Test Accuracy 0.627/0.627 | LR 1.00e-03 | Time 171.1m\n",
      "Batch Number 0\n",
      "Batch Number 100\n",
      "Batch Number 200\n",
      "Batch Number 300\n",
      "Batch Number 400\n",
      "Batch Number 500\n",
      "Batch Number 600\n",
      "Batch Number 700\n",
      "Ep 07 | Train 0.939/0.667 | Val 0.984/0.652 | Train Accuracy 0.667/0.667 | Test Accuracy 0.652/0.652 | LR 1.00e-03 | Time 197.1m\n",
      "Batch Number 0\n",
      "Batch Number 100\n",
      "Batch Number 200\n",
      "Batch Number 300\n",
      "Batch Number 400\n",
      "Batch Number 500\n",
      "Batch Number 600\n",
      "Batch Number 700\n",
      "Ep 08 | Train 0.884/0.688 | Val 0.975/0.652 | Train Accuracy 0.688/0.688 | Test Accuracy 0.652/0.652 | LR 1.00e-03 | Time 223.0m\n",
      "Batch Number 0\n",
      "Batch Number 100\n",
      "Batch Number 200\n",
      "Batch Number 300\n",
      "Batch Number 400\n",
      "Batch Number 500\n",
      "Batch Number 600\n",
      "Batch Number 700\n",
      "Ep 09 | Train 0.857/0.698 | Val 0.946/0.664 | Train Accuracy 0.698/0.698 | Test Accuracy 0.664/0.664 | LR 1.00e-03 | Time 251.8m\n",
      "Batch Number 0\n",
      "Batch Number 100\n",
      "Batch Number 200\n",
      "Batch Number 300\n",
      "Batch Number 400\n",
      "Batch Number 500\n",
      "Batch Number 600\n",
      "Batch Number 700\n",
      "Ep 10 | Train 0.821/0.709 | Val 0.929/0.679 | Train Accuracy 0.709/0.709 | Test Accuracy 0.679/0.679 | LR 1.00e-03 | Time 278.8m\n",
      "Batch Number 0\n",
      "Batch Number 100\n",
      "Batch Number 200\n",
      "Batch Number 300\n",
      "Batch Number 400\n",
      "Batch Number 500\n",
      "Batch Number 600\n",
      "Batch Number 700\n",
      "Ep 11 | Train 0.784/0.723 | Val 0.926/0.674 | Train Accuracy 0.723/0.723 | Test Accuracy 0.674/0.674 | LR 1.00e-03 | Time 305.0m\n",
      "Batch Number 0\n",
      "Batch Number 100\n",
      "Batch Number 200\n",
      "Batch Number 300\n",
      "Batch Number 400\n",
      "Batch Number 500\n",
      "Batch Number 600\n",
      "Batch Number 700\n",
      "Ep 12 | Train 0.753/0.734 | Val 0.892/0.683 | Train Accuracy 0.734/0.734 | Test Accuracy 0.683/0.683 | LR 1.00e-03 | Time 331.7m\n",
      "Batch Number 0\n",
      "Batch Number 100\n",
      "Batch Number 200\n",
      "Batch Number 300\n",
      "Batch Number 400\n",
      "Batch Number 500\n",
      "Batch Number 600\n",
      "Batch Number 700\n",
      "Ep 13 | Train 0.733/0.742 | Val 0.894/0.685 | Train Accuracy 0.742/0.742 | Test Accuracy 0.685/0.685 | LR 1.00e-03 | Time 357.8m\n",
      "Batch Number 0\n",
      "Batch Number 100\n",
      "Batch Number 200\n",
      "Batch Number 300\n",
      "Batch Number 400\n",
      "Batch Number 500\n",
      "Batch Number 600\n",
      "Batch Number 700\n",
      "Ep 14 | Train 0.708/0.752 | Val 0.957/0.665 | Train Accuracy 0.752/0.752 | Test Accuracy 0.665/0.665 | LR 1.00e-03 | Time 383.9m\n",
      "Batch Number 0\n",
      "Batch Number 100\n",
      "Batch Number 200\n",
      "Batch Number 300\n",
      "Batch Number 400\n",
      "Batch Number 500\n",
      "Batch Number 600\n",
      "Batch Number 700\n",
      "Ep 15 | Train 0.684/0.760 | Val 0.881/0.695 | Train Accuracy 0.760/0.760 | Test Accuracy 0.695/0.695 | LR 1.00e-03 | Time 409.3m\n",
      "Batch Number 0\n",
      "Batch Number 100\n",
      "Batch Number 200\n",
      "Batch Number 300\n",
      "Batch Number 400\n",
      "Batch Number 500\n",
      "Batch Number 600\n",
      "Batch Number 700\n",
      "Ep 16 | Train 0.664/0.766 | Val 0.882/0.691 | Train Accuracy 0.766/0.766 | Test Accuracy 0.691/0.691 | LR 1.00e-03 | Time 437.8m\n",
      "Batch Number 0\n",
      "Batch Number 100\n",
      "Batch Number 200\n",
      "Batch Number 300\n",
      "Batch Number 400\n",
      "Batch Number 500\n",
      "Batch Number 600\n",
      "Batch Number 700\n",
      "Ep 17 | Train 0.643/0.774 | Val 0.866/0.701 | Train Accuracy 0.774/0.774 | Test Accuracy 0.701/0.701 | LR 1.00e-03 | Time 467.1m\n",
      "Batch Number 0\n",
      "Batch Number 100\n",
      "Batch Number 200\n",
      "Batch Number 300\n",
      "Batch Number 400\n",
      "Batch Number 500\n",
      "Batch Number 600\n",
      "Batch Number 700\n",
      "Ep 18 | Train 0.620/0.783 | Val 0.854/0.712 | Train Accuracy 0.783/0.783 | Test Accuracy 0.712/0.712 | LR 1.00e-03 | Time 494.3m\n",
      "Batch Number 0\n",
      "Batch Number 100\n",
      "Batch Number 200\n",
      "Batch Number 300\n",
      "Batch Number 400\n",
      "Batch Number 500\n",
      "Batch Number 600\n",
      "Batch Number 700\n",
      "Ep 19 | Train 0.602/0.790 | Val 0.878/0.701 | Train Accuracy 0.790/0.790 | Test Accuracy 0.701/0.701 | LR 1.00e-03 | Time 520.8m\n",
      "Batch Number 0\n",
      "Batch Number 100\n",
      "Batch Number 200\n",
      "Batch Number 300\n",
      "Batch Number 400\n",
      "Batch Number 500\n",
      "Batch Number 600\n",
      "Batch Number 700\n",
      "Ep 20 | Train 0.587/0.795 | Val 0.843/0.708 | Train Accuracy 0.795/0.795 | Test Accuracy 0.708/0.708 | LR 1.00e-03 | Time 546.6m\n",
      "Batch Number 0\n",
      "Batch Number 100\n",
      "Batch Number 200\n",
      "Batch Number 300\n",
      "Batch Number 400\n",
      "Batch Number 500\n",
      "Batch Number 600\n",
      "Batch Number 700\n",
      "Ep 21 | Train 0.565/0.802 | Val 0.893/0.692 | Train Accuracy 0.802/0.802 | Test Accuracy 0.692/0.692 | LR 1.00e-03 | Time 572.3m\n",
      "Batch Number 0\n",
      "Batch Number 100\n",
      "Batch Number 200\n",
      "Batch Number 300\n",
      "Batch Number 400\n",
      "Batch Number 500\n",
      "Batch Number 600\n",
      "Batch Number 700\n",
      "Ep 22 | Train 0.735/0.751 | Val 0.963/0.665 | Train Accuracy 0.751/0.751 | Test Accuracy 0.665/0.665 | LR 1.00e-03 | Time 598.5m\n",
      "Batch Number 0\n",
      "Batch Number 100\n",
      "Batch Number 200\n",
      "Batch Number 300\n",
      "Batch Number 400\n",
      "Batch Number 500\n",
      "Batch Number 600\n",
      "Batch Number 700\n",
      "Ep 23 | Train 0.925/0.673 | Val 0.904/0.688 | Train Accuracy 0.673/0.673 | Test Accuracy 0.688/0.688 | LR 5.00e-04 | Time 624.4m\n",
      "Batch Number 0\n",
      "Batch Number 100\n",
      "Batch Number 200\n",
      "Batch Number 300\n",
      "Batch Number 400\n",
      "Batch Number 500\n",
      "Batch Number 600\n",
      "Batch Number 700\n",
      "Ep 24 | Train 0.657/0.770 | Val 0.866/0.703 | Train Accuracy 0.770/0.770 | Test Accuracy 0.703/0.703 | LR 5.00e-04 | Time 650.5m\n",
      "Batch Number 0\n",
      "Batch Number 100\n",
      "Batch Number 200\n",
      "Batch Number 300\n",
      "Batch Number 400\n",
      "Batch Number 500\n",
      "Batch Number 600\n",
      "Batch Number 700\n",
      "Ep 25 | Train 0.596/0.792 | Val 0.871/0.703 | Train Accuracy 0.792/0.792 | Test Accuracy 0.703/0.703 | LR 5.00e-04 | Time 677.4m\n",
      "Batch Number 0\n",
      "Batch Number 100\n",
      "Batch Number 200\n",
      "Batch Number 300\n",
      "Batch Number 400\n",
      "Batch Number 500\n",
      "Batch Number 600\n",
      "Batch Number 700\n",
      "Ep 26 | Train 0.560/0.805 | Val 0.846/0.714 | Train Accuracy 0.805/0.805 | Test Accuracy 0.714/0.714 | LR 2.50e-04 | Time 703.9m\n",
      "Batch Number 0\n",
      "Batch Number 100\n",
      "Batch Number 200\n",
      "Batch Number 300\n",
      "Batch Number 400\n",
      "Batch Number 500\n",
      "Batch Number 600\n",
      "Batch Number 700\n",
      "Ep 27 | Train 0.498/0.830 | Val 0.839/0.723 | Train Accuracy 0.830/0.830 | Test Accuracy 0.723/0.723 | LR 2.50e-04 | Time 730.3m\n",
      "Batch Number 0\n",
      "Batch Number 100\n",
      "Batch Number 200\n",
      "Batch Number 300\n",
      "Batch Number 400\n",
      "Batch Number 500\n",
      "Batch Number 600\n",
      "Batch Number 700\n",
      "Ep 28 | Train 0.480/0.836 | Val 0.850/0.718 | Train Accuracy 0.836/0.836 | Test Accuracy 0.718/0.718 | LR 2.50e-04 | Time 756.8m\n",
      "Batch Number 0\n",
      "Batch Number 100\n",
      "Batch Number 200\n",
      "Batch Number 300\n",
      "Batch Number 400\n",
      "Batch Number 500\n",
      "Batch Number 600\n",
      "Batch Number 700\n",
      "Ep 29 | Train 0.467/0.840 | Val 0.849/0.721 | Train Accuracy 0.840/0.840 | Test Accuracy 0.721/0.721 | LR 2.50e-04 | Time 784.3m\n",
      "Batch Number 0\n",
      "Batch Number 100\n",
      "Batch Number 200\n",
      "Batch Number 300\n",
      "Batch Number 400\n",
      "Batch Number 500\n",
      "Batch Number 600\n",
      "Batch Number 700\n",
      "Ep 30 | Train 0.454/0.845 | Val 0.866/0.715 | Train Accuracy 0.845/0.845 | Test Accuracy 0.715/0.715 | LR 1.25e-04 | Time 813.3m\n",
      "Batch Number 0\n",
      "Batch Number 100\n",
      "Batch Number 200\n",
      "Batch Number 300\n",
      "Batch Number 400\n",
      "Batch Number 500\n",
      "Batch Number 600\n",
      "Batch Number 700\n",
      "Ep 31 | Train 0.429/0.855 | Val 0.871/0.715 | Train Accuracy 0.855/0.855 | Test Accuracy 0.715/0.715 | LR 1.25e-04 | Time 840.7m\n",
      "Batch Number 0\n",
      "Batch Number 100\n",
      "Batch Number 200\n",
      "Batch Number 300\n",
      "Batch Number 400\n",
      "Batch Number 500\n",
      "Batch Number 600\n",
      "Batch Number 700\n",
      "Ep 32 | Train 0.416/0.861 | Val 0.881/0.718 | Train Accuracy 0.861/0.861 | Test Accuracy 0.718/0.718 | LR 1.25e-04 | Time 868.1m\n",
      "Batch Number 0\n",
      "Batch Number 100\n",
      "Batch Number 200\n",
      "Batch Number 300\n",
      "Batch Number 400\n",
      "Batch Number 500\n",
      "Batch Number 600\n",
      "Batch Number 700\n",
      "Ep 33 | Train 0.414/0.861 | Val 0.857/0.723 | Train Accuracy 0.861/0.861 | Test Accuracy 0.723/0.723 | LR 6.25e-05 | Time 894.0m\n",
      "Batch Number 0\n",
      "Batch Number 100\n",
      "Batch Number 200\n",
      "Batch Number 300\n",
      "Batch Number 400\n",
      "Batch Number 500\n",
      "Batch Number 600\n",
      "Batch Number 700\n",
      "Ep 34 | Train 0.395/0.869 | Val 0.858/0.723 | Train Accuracy 0.869/0.869 | Test Accuracy 0.723/0.723 | LR 6.25e-05 | Time 920.1m\n",
      "Batch Number 0\n",
      "Batch Number 100\n",
      "Batch Number 200\n",
      "Batch Number 300\n",
      "Batch Number 400\n",
      "Batch Number 500\n",
      "Batch Number 600\n",
      "Batch Number 700\n",
      "Ep 35 | Train 0.394/0.869 | Val 0.875/0.724 | Train Accuracy 0.869/0.869 | Test Accuracy 0.724/0.724 | LR 6.25e-05 | Time 947.0m\n",
      "Batch Number 0\n",
      "Batch Number 100\n",
      "Batch Number 200\n",
      "Batch Number 300\n",
      "Batch Number 400\n",
      "Batch Number 500\n",
      "Batch Number 600\n",
      "Batch Number 700\n",
      "Ep 36 | Train 0.389/0.871 | Val 0.871/0.723 | Train Accuracy 0.871/0.871 | Test Accuracy 0.723/0.723 | LR 3.13e-05 | Time 973.2m\n",
      "Batch Number 0\n",
      "Batch Number 100\n",
      "Batch Number 200\n",
      "Batch Number 300\n",
      "Batch Number 400\n",
      "Batch Number 500\n",
      "Batch Number 600\n",
      "Batch Number 700\n",
      "Ep 37 | Train 0.377/0.876 | Val 0.862/0.725 | Train Accuracy 0.876/0.876 | Test Accuracy 0.725/0.725 | LR 3.13e-05 | Time 999.4m\n",
      "Batch Number 0\n",
      "Batch Number 100\n",
      "Batch Number 200\n",
      "Batch Number 300\n",
      "Batch Number 400\n",
      "Batch Number 500\n",
      "Batch Number 600\n",
      "Batch Number 700\n",
      "Ep 38 | Train 0.379/0.875 | Val 0.875/0.722 | Train Accuracy 0.875/0.875 | Test Accuracy 0.722/0.722 | LR 3.13e-05 | Time 1025.6m\n",
      "Batch Number 0\n",
      "Batch Number 100\n",
      "Batch Number 200\n",
      "Batch Number 300\n",
      "Batch Number 400\n",
      "Batch Number 500\n",
      "Batch Number 600\n",
      "Batch Number 700\n",
      "Ep 39 | Train 0.381/0.874 | Val 0.883/0.718 | Train Accuracy 0.874/0.874 | Test Accuracy 0.718/0.718 | LR 1.56e-05 | Time 1051.6m\n",
      "Batch Number 0\n",
      "Batch Number 100\n",
      "Batch Number 200\n",
      "Batch Number 300\n",
      "Batch Number 400\n",
      "Batch Number 500\n",
      "Batch Number 600\n",
      "Batch Number 700\n",
      "Ep 40 | Train 0.374/0.877 | Val 0.865/0.728 | Train Accuracy 0.877/0.877 | Test Accuracy 0.728/0.728 | LR 1.56e-05 | Time 1077.9m\n",
      "Batch Number 0\n"
     ]
    }
   ],
   "source": [
    "# Chebyshev Convolutional NN , separate activation per layer\n",
    "import math\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.optim as optim\n",
    "from pathlib import Path\n",
    "\n",
    "import torch._dynamo\n",
    "torch._dynamo.config.suppress_errors = True\n",
    "\n",
    "torch.set_num_threads(8)         \n",
    "torch.backends.mkldnn.enabled = True\n",
    "device = torch.device(\"cpu\")      \n",
    "dtype = torch.float32        \n",
    "\n",
    "def cheb_nodes_first_kind(n, a=-1.0, b=1.0):\n",
    "    k = torch.arange(1, n + 1, dtype=dtype)\n",
    "    x = torch.cos((2 * k - 1) * math.pi / (2 * n))\n",
    "    return 0.5 * (a + b) + 0.5 * (b - a) * x\n",
    "\n",
    "def cheb_nodes_second_kind(n, a=-1.0, b=1.0):\n",
    "    k = torch.arange(0, n, dtype=dtype)\n",
    "    x = torch.cos(k * math.pi / (n - 1))\n",
    "    return 0.5 * (a + b) + 0.5 * (b - a) * x\n",
    "\n",
    "class InputScaler(nn.Module):\n",
    "    def __init__(self, momentum=0.9):\n",
    "        super().__init__()\n",
    "        self.momentum = momentum\n",
    "        self.register_buffer(\"running_min\", torch.tensor(0., dtype=dtype))\n",
    "        self.register_buffer(\"running_max\", torch.tensor(0., dtype=dtype))\n",
    "        self.register_buffer(\"initialized\", torch.tensor(False))\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update_stats(self, x):\n",
    "        batch_min, batch_max = x.min(), x.max()\n",
    "        if not self.initialized:\n",
    "            self.running_min.copy_(batch_min)\n",
    "            self.running_max.copy_(batch_max)\n",
    "            self.initialized.fill_(True)\n",
    "        else:\n",
    "            self.running_min.mul_(self.momentum).add_(batch_min * (1 - self.momentum))\n",
    "            self.running_max.mul_(self.momentum).add_(batch_max * (1 - self.momentum))\n",
    "\n",
    "    def scale(self, x, training=True):\n",
    "        if training and self.training:\n",
    "            self.update_stats(x)\n",
    "\n",
    "        if not self.initialized:\n",
    "            min_val, max_val = x.min(), x.max()\n",
    "        else:\n",
    "            min_val, max_val = self.running_min, self.running_max\n",
    "\n",
    "        range_val = torch.clamp(max_val - min_val, min=1e-8)\n",
    "        scaled = 2.0 * (x - min_val) / range_val - 1.0\n",
    "        return torch.clamp(scaled, -1.0, 1.0)\n",
    "\n",
    "class SharedChebyshevLayer(nn.Module):\n",
    "    def __init__(self, feature_size, degree, use_nodes=True, node_kind=\"first\"):\n",
    "        super().__init__()\n",
    "        self.degree = degree\n",
    "        self.feature_size = feature_size\n",
    "        self.coeffs = nn.Parameter(torch.randn(degree + 1, dtype=dtype) * 0.01)\n",
    "        self.scaler = InputScaler()\n",
    "        \n",
    "        self.register_buffer(\"nodes\",\n",
    "            cheb_nodes_first_kind(degree+1) if use_nodes and node_kind==\"first\"\n",
    "            else cheb_nodes_second_kind(degree+1) if use_nodes else torch.linspace(-1,1,degree+1)\n",
    "        )\n",
    "\n",
    "    def _scale_inputs(self, x, training=True):\n",
    "        return self.scaler.scale(x, training)\n",
    "\n",
    "    def _basis(self, x):\n",
    "        T0, T1 = torch.ones_like(x), x\n",
    "        basis = [T0, T1]\n",
    "        for k in range(1, self.degree):\n",
    "            Tk = 2.0 * x * basis[-1] - basis[-2]\n",
    "            if k < self.nodes.numel():\n",
    "                node_w = torch.exp(-0.5 * torch.abs(x - self.nodes[k]))\n",
    "                Tk *= node_w\n",
    "            basis.append(Tk)\n",
    "        return torch.stack(basis[:self.degree+1], dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        xs = self._scale_inputs(x, self.training)\n",
    "        \n",
    "        if xs.dim() == 4: \n",
    "            B, C, H, W = xs.shape\n",
    "            xs_flat = xs.view(B, C, H*W)\n",
    "            Phi = self._basis(xs_flat) \n",
    "            y = torch.einsum(\"bchk,k->bch\", Phi, self.coeffs)\n",
    "            return y.view(B, C, H, W)\n",
    "        else: \n",
    "            Phi = self._basis(xs)  \n",
    "            return torch.einsum(\"bfk,k->bf\", Phi, self.coeffs)\n",
    "\n",
    "class BottleneckBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, reduction_ratio=4):\n",
    "        super().__init__()\n",
    "        mid = max(in_ch // reduction_ratio, 8)\n",
    "        self.reduce = nn.Conv2d(in_ch, mid, 1, bias=False)\n",
    "        self.conv = nn.Conv2d(mid, mid, 3, padding=1, bias=False)\n",
    "        self.expand = nn.Conv2d(mid, out_ch, 1, bias=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.expand(self.conv(self.reduce(x)))\n",
    "\n",
    "class ConvolutionalChebyshevNN(nn.Module):\n",
    "    def __init__(self, num_classes=10, conv_depth=5,\n",
    "                 cheb_degree=20, use_nodes=True, node_kind=\"second\", use_bottleneck=True):\n",
    "        super().__init__()\n",
    "        ch = [3, 32, 64, 128, 256, 384] if use_bottleneck else [3,48,96,192,320,512]\n",
    "        self.conv_layers, self.cheb_layers, self.norms = nn.ModuleList(), nn.ModuleList(), nn.ModuleList()\n",
    "        self.pool, self.pool_after = nn.MaxPool2d(2), {1,2,3,4}\n",
    "\n",
    "        for i in range(conv_depth):\n",
    "            self.conv_layers.append(\n",
    "                BottleneckBlock(ch[i],ch[i+1]) if use_bottleneck and i>0\n",
    "                else nn.Conv2d(ch[i],ch[i+1],3,padding=1,bias=False)\n",
    "            )\n",
    "            self.cheb_layers.append(\n",
    "                SharedChebyshevLayer(ch[i+1], cheb_degree, use_nodes, node_kind)\n",
    "            )\n",
    "            self.norms.append(nn.LayerNorm(ch[i+1]))\n",
    "\n",
    "        self.final_cheb = SharedChebyshevLayer(ch[-1], cheb_degree, use_nodes, node_kind)\n",
    "        self.heads = nn.ModuleList([nn.Linear(ch[-1],1) for _ in range(num_classes)])\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    @staticmethod\n",
    "    def _init_weights(m):\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            nn.init.kaiming_normal_(m.weight, mode=\"fan_out\")\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            nn.init.kaiming_normal_(m.weight, mode=\"fan_in\")\n",
    "            nn.init.zeros_(m.bias)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.ones_(m.weight); nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(memory_format=torch.channels_last)\n",
    "        for i,(cv,chb,ln) in enumerate(zip(self.conv_layers,self.cheb_layers,self.norms)):\n",
    "            x = cv(x)\n",
    "            x = chb(x)\n",
    "            x = ln(x.permute(0,2,3,1)).permute(0,3,1,2)\n",
    "            if i in self.pool_after: x = self.pool(x)\n",
    "        x = F.adaptive_avg_pool2d(x,1).flatten(1)\n",
    "        x = F.normalize(self.final_cheb(x),p=2,dim=1,eps=1e-6)\n",
    "        return torch.cat([h(x) for h in self.heads], dim=1)\n",
    "\n",
    "def count_parameters(model):\n",
    "    tot = sum(p.numel() for p in model.parameters())\n",
    "    train = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return tot, train\n",
    "\n",
    "# def compile_net(net):\n",
    "#     try:        \n",
    "#         return torch.compile(net, mode=\"default\", dynamic=True)\n",
    "#     except Exception:\n",
    "#         return torch.jit.script(net)\n",
    "\n",
    "def train(model, train_images, train_labels, test_images, test_labels, epochs=50, bs=64, patience=5):\n",
    "    train_loader = DataLoader(TensorDataset(train_images, train_labels), bs , shuffle=True, num_workers=8,\n",
    "                              pin_memory=False, persistent_workers=True)\n",
    "    test_loader = DataLoader(TensorDataset(test_images, test_labels), bs , shuffle=False,num_workers=8,\n",
    "                              pin_memory=False, persistent_workers=True)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    opt = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(opt, mode=\"min\",\n",
    "                                                     patience=2, factor=0.5, min_lr=1e-5)\n",
    "    best_val, wait = 1e9, 0\n",
    "    start = time.time()\n",
    "\n",
    "    for ep in range(1, epochs+1):\n",
    "        model.train()\n",
    "        tr_loss, tr_hit = 0., 0\n",
    "        for i, (x, y) in enumerate(train_loader):\n",
    "            if i%100 ==0:\n",
    "                print(f'Batch Number {i}')\n",
    "            x,y = x.to(device,dtype), y.to(device)\n",
    "            opt.zero_grad()\n",
    "            out = model(x)\n",
    "            loss = criterion(out,y)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(),1.0)\n",
    "            opt.step()\n",
    "\n",
    "            tr_loss += loss.item()*x.size(0)\n",
    "            tr_hit += (out.argmax(1)==y).sum().item()\n",
    "\n",
    "        model.eval()\n",
    "        vl_loss, vl_hit = 0., 0\n",
    "        with torch.no_grad():\n",
    "            for x,y in test_loader:\n",
    "                x,y = x.to(device,dtype), y.to(device)\n",
    "                out = model(x)\n",
    "                vl_loss += criterion(out,y).item()*x.size(0)\n",
    "                vl_hit += (out.argmax(1)==y).sum().item()\n",
    "\n",
    "        tr_loss /= len(train_loader.dataset); tr_acc = tr_hit/len(train_loader.dataset)\n",
    "        vl_loss /= len(test_loader.dataset) ; vl_acc = vl_hit/len(test_loader.dataset)\n",
    "        scheduler.step(vl_loss)\n",
    "        \n",
    "        print(f\"Ep {ep:02d} | \"\n",
    "              f\"Train {tr_loss:.3f}/{tr_acc:.3f} | \"\n",
    "              f\"Val {vl_loss:.3f}/{vl_acc:.3f} | \"\n",
    "              f\"Train Accuracy {tr_acc:.3f}/{tr_acc:.3f} | \"\n",
    "              f\"Test Accuracy {vl_acc:.3f}/{vl_acc:.3f} | \"\n",
    "              f\"LR {opt.param_groups[0]['lr']:.2e} | \"\n",
    "              f\"Time {(time.time()-start)/60:.1f}m\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    net = ConvolutionalChebyshevNN(num_classes=10, cheb_degree=10, use_bottleneck=True)\n",
    "    print(f\"Total params: {count_parameters(net)[0]:,}\")\n",
    "    train(net, train_images, train_labels, test_images, test_labels, epochs=100, bs=64, patience=1)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb99060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chebyshev Convolutional NN , separate activation per feature element per layer \n",
    "# This Architechture of NN does not exist Already in traditional NN , but it is compute extensive\n",
    "# Also different weights and biases for different classes can be another strategy along with Chebyshev Activations \n",
    "# Etc ... can  be tried to increase the expressivity of the Conventional NN further , with ultimate goal to construct compact\n",
    "# human understanable equations from these obtained NN equations , i.e. first use unsupervised learning to reduce the number \n",
    "# of features and then make the interpreted equations human understandable on this compact manifold , and then scale them back to given feature size...\n",
    "# Universal Audio , Image , Video , Text best P(Y/X) per class human understandable\n",
    "# equations or all the terms of Bayes Equations per class is another ultimate scope of this research...this can lead to further innovations in the field\n",
    "# and then extending this research from Supervised or Generative or Unsuperivised(Autoencoder as done in Bayes Theorem) to other types of NN Architechtures \n",
    "# namely Reinforcement Learning and World Models...\n",
    "\n",
    "\n",
    "import math, time, torch, torch.nn as nn, torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.optim as optim\n",
    "from pathlib import Path\n",
    "\n",
    "import torch._dynamo\n",
    "torch._dynamo.config.suppress_errors = True\n",
    "\n",
    "\n",
    "torch.set_num_threads(8)         \n",
    "torch.backends.mkldnn.enabled = True\n",
    "device = torch.device(\"cpu\")      \n",
    "dtype   = torch.float32        \n",
    "\n",
    "\n",
    "def cheb_nodes_first_kind(n, a=-1.0, b=1.0):\n",
    "    k = torch.arange(1, n + 1, dtype=dtype)\n",
    "    x = torch.cos((2 * k - 1) * math.pi / (2 * n))\n",
    "    return 0.5 * (a + b) + 0.5 * (b - a) * x\n",
    "\n",
    "def cheb_nodes_second_kind(n, a=-1.0, b=1.0):\n",
    "    k = torch.arange(0, n, dtype=dtype)\n",
    "    x = torch.cos(k * math.pi / (n - 1))\n",
    "    return 0.5 * (a + b) + 0.5 * (b - a) * x\n",
    "\n",
    "class InputScaler(nn.Module):\n",
    "    def __init__(self, momentum=0.9):\n",
    "        super().__init__()\n",
    "        self.momentum = momentum\n",
    "        self.register_buffer(\"running_min\", torch.tensor(0., dtype=dtype))\n",
    "        self.register_buffer(\"running_max\", torch.tensor(0., dtype=dtype))\n",
    "        self.register_buffer(\"initialized\", torch.tensor(False))\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update_stats(self, x):\n",
    "        batch_min, batch_max = x.min(), x.max()\n",
    "        if not self.initialized:\n",
    "            self.running_min.copy_(batch_min)\n",
    "            self.running_max.copy_(batch_max)\n",
    "            self.initialized.fill_(True)\n",
    "        else:\n",
    "            self.running_min.mul_(self.momentum).add_(batch_min * (1 - self.momentum))\n",
    "            self.running_max.mul_(self.momentum).add_(batch_max * (1 - self.momentum))\n",
    "\n",
    "    def scale(self, x, training=True):\n",
    "        if training and self.training:\n",
    "            self.update_stats(x)\n",
    "\n",
    "        if not self.initialized:\n",
    "            min_val, max_val = x.min(), x.max()\n",
    "        else:\n",
    "            min_val, max_val = self.running_min, self.running_max\n",
    "\n",
    "        range_val = torch.clamp(max_val - min_val, min=1e-8)\n",
    "        scaled = 2.0 * (x - min_val) / range_val - 1.0\n",
    "        return torch.clamp(scaled, -1.0, 1.0)\n",
    "\n",
    "class SharedChebyshevLayer(nn.Module):\n",
    "    def __init__(self, feature_size, degree, use_nodes=True, node_kind=\"first\"):\n",
    "        super().__init__()\n",
    "        self.degree, self.feature_size = degree, feature_size\n",
    "        self.coeffs = nn.Parameter(torch.randn(feature_size, degree + 1, dtype=dtype)*0.01)\n",
    "\n",
    "        self.scalers = nn.ModuleList([InputScaler() for _ in range(feature_size)])\n",
    "        self.register_buffer(\"nodes\",\n",
    "            cheb_nodes_first_kind(degree+1) if use_nodes and node_kind==\"first\"\n",
    "            else cheb_nodes_second_kind(degree+1) if use_nodes else torch.linspace(-1,1,degree+1)\n",
    "        )\n",
    "\n",
    "    def _scale_inputs(self, x, training=True):\n",
    "        if x.dim() == 4:            # (B,C,H,W)\n",
    "            B,C,H,W = x.shape\n",
    "            x_flat  = x.view(B, C, -1)\n",
    "            scaled  = torch.zeros_like(x_flat)\n",
    "            for c in range(C):\n",
    "                scaled[:,c,:] = self.scalers[c].scale(x_flat[:,c,:], training)\n",
    "            return scaled.view(B,C,H,W)\n",
    "        else:                       # (B,F)\n",
    "            out = torch.zeros_like(x)\n",
    "            for f in range(x.size(1)):\n",
    "                out[:,f] = self.scalers[f].scale(x[:,f], training)\n",
    "            return out\n",
    "\n",
    "    def _basis(self, x):\n",
    "        T0, T1 = torch.ones_like(x), x\n",
    "        basis  = [T0, T1]\n",
    "        for k in range(1, self.degree):\n",
    "            Tk = 2.0 * x * basis[-1] - basis[-2]\n",
    "            if k < self.nodes.numel():\n",
    "                node_w = torch.exp(-0.5 * torch.abs(x - self.nodes[k]))\n",
    "                Tk *= node_w\n",
    "            basis.append(Tk)\n",
    "        return torch.stack(basis[:self.degree+1], dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        xs = self._scale_inputs(x, self.training)\n",
    "        if xs.dim()==4:\n",
    "            B, C, H, W = xs.shape\n",
    "            Phi = self._basis(xs.view(B,C,-1))\n",
    "            y   = torch.einsum(\"bchk,ck->bch\", Phi, self.coeffs)\n",
    "            return y.view(B,C,H,W)\n",
    "        else:\n",
    "            Phi = self._basis(xs)\n",
    "            return torch.einsum(\"bfk,fk->bf\", Phi, self.coeffs)\n",
    "\n",
    "class BottleneckBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, reduction_ratio=4):\n",
    "        super().__init__()\n",
    "        mid = max(in_ch // reduction_ratio, 8)\n",
    "        self.reduce = nn.Conv2d(in_ch, mid, 1, bias=False)\n",
    "        self.conv   = nn.Conv2d(mid, mid, 3, padding=1, bias=False)\n",
    "        self.expand = nn.Conv2d(mid, out_ch, 1, bias=False)\n",
    "    def forward(self, x):\n",
    "        return self.expand(self.conv(self.reduce(x)))\n",
    "\n",
    "class ConvolutionalChebyshevNN(nn.Module):\n",
    "    def __init__(self, num_classes=10, conv_depth=5,\n",
    "                 cheb_degree=20, use_nodes=True, node_kind=\"second\", use_bottleneck=True):\n",
    "        super().__init__()\n",
    "        ch = [3, 32, 64, 128, 256, 384] if use_bottleneck else [3,48,96,192,320,512]\n",
    "        self.conv_layers, self.cheb_layers, self.norms = nn.ModuleList(), nn.ModuleList(), nn.ModuleList()\n",
    "        self.pool, self.pool_after = nn.MaxPool2d(2), {1,2,3,4}\n",
    "\n",
    "        for i in range(conv_depth):\n",
    "            self.conv_layers.append(\n",
    "                BottleneckBlock(ch[i],ch[i+1]) if use_bottleneck and i>0\n",
    "                else nn.Conv2d(ch[i],ch[i+1],3,padding=1,bias=False)\n",
    "            )\n",
    "            self.cheb_layers.append(\n",
    "                SharedChebyshevLayer(ch[i+1], cheb_degree, use_nodes, node_kind)\n",
    "            )\n",
    "            self.norms.append(nn.LayerNorm(ch[i+1]))\n",
    "\n",
    "        self.final_cheb = SharedChebyshevLayer(ch[-1], cheb_degree, use_nodes, node_kind)\n",
    "        self.heads = nn.ModuleList([nn.Linear(ch[-1],1) for _ in range(num_classes)])\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    @staticmethod\n",
    "    def _init_weights(m):\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            nn.init.kaiming_normal_(m.weight, mode=\"fan_out\")\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            nn.init.kaiming_normal_(m.weight, mode=\"fan_in\")\n",
    "            nn.init.zeros_(m.bias)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.ones_(m.weight); nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(memory_format=torch.channels_last)\n",
    "        for i,(cv,chb,ln) in enumerate(zip(self.conv_layers,self.cheb_layers,self.norms)):\n",
    "            x = cv(x)\n",
    "            x = chb(x)\n",
    "            x = ln(x.permute(0,2,3,1)).permute(0,3,1,2)\n",
    "            if i in self.pool_after: x = self.pool(x)\n",
    "        x = F.adaptive_avg_pool2d(x,1).flatten(1)\n",
    "        x = F.normalize(self.final_cheb(x),p=2,dim=1,eps=1e-6)\n",
    "        return torch.cat([h(x) for h in self.heads], dim=1)\n",
    "\n",
    "def count_parameters(model):\n",
    "    tot = sum(p.numel() for p in model.parameters())\n",
    "    train = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return tot, train\n",
    "\n",
    "def compile_net(net):\n",
    "    try:        \n",
    "        return torch.compile(net, mode=\"default\", dynamic=True)\n",
    "    except Exception:\n",
    "        return torch.jit.script(net)\n",
    "\n",
    "def train(model, train_images,train_labels, test_images,test_labels, epochs=50, bs=64, patience=5):\n",
    "\n",
    "    train_loader = DataLoader(TensorDataset(train_images, train_labels), bs , shuffle=True, num_workers=8,\n",
    "                              pin_memory=False, persistent_workers=True)\n",
    "    test_loader  = DataLoader(TensorDataset(test_images, test_labels), bs , shuffle=False,num_workers=8,\n",
    "                              pin_memory=False, persistent_workers=True)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    opt = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(opt, mode=\"min\",\n",
    "                                                     patience=2, factor=0.5, min_lr=1e-5)\n",
    "    best_val, wait = 1e9, 0\n",
    "    start = time.time()\n",
    "\n",
    "    for ep in range(1, epochs+1):\n",
    "        model.train()\n",
    "        tr_loss, tr_hit = 0., 0\n",
    "        for i, (x, y) in enumerate(train_loader):\n",
    "            print(f'Batch Number {i}')\n",
    "            x,y = x.to(device,dtype), y.to(device)\n",
    "            opt.zero_grad()\n",
    "            out = model(x)\n",
    "            loss = criterion(out,y)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(),1.0)\n",
    "            opt.step()\n",
    "\n",
    "            tr_loss += loss.item()*x.size(0)\n",
    "            tr_hit  += (out.argmax(1)==y).sum().item()\n",
    "\n",
    "        model.eval()\n",
    "        vl_loss, vl_hit = 0., 0\n",
    "        with torch.no_grad():\n",
    "            for x,y in test_loader:\n",
    "                x,y = x.to(device,dtype), y.to(device)\n",
    "                out = model(x)\n",
    "                vl_loss += criterion(out,y).item()*x.size(0)\n",
    "                vl_hit  += (out.argmax(1)==y).sum().item()\n",
    "\n",
    "        tr_loss /= len(train_loader.dataset); tr_acc = tr_hit/len(train_loader.dataset)\n",
    "        vl_loss /= len(test_loader.dataset) ; vl_acc = vl_hit/len(test_loader.dataset)\n",
    "        scheduler.step(vl_loss)\n",
    "\n",
    "        print(f\"Ep {ep:02d} | \"\n",
    "              f\"Train {tr_loss:.3f}/{tr_acc:.3f} | \"\n",
    "              f\"Val {vl_loss:.3f}/{vl_acc:.3f} | \"\n",
    "              f\"LR {opt.param_groups[0]['lr']:.2e} | \"\n",
    "              f\"Time {(time.time()-start)/60:.1f}m\")\n",
    "        if vl_loss < best_val:\n",
    "            best_val, wait = vl_loss, 0\n",
    "            torch.save(model.state_dict(), \"best_cheb_cpu.pth\")\n",
    "        else:\n",
    "            wait += 1\n",
    "            if wait > patience:\n",
    "                print(\"Early stopping.\")\n",
    "                break\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    net = ConvolutionalChebyshevNN(num_classes=10, cheb_degree=20, use_bottleneck=False)\n",
    "    net = compile_net(net).to(device)\n",
    "    print(f\"Total params: {count_parameters(net)[0]:,}\")\n",
    "\n",
    "    train(net, train_images, train_labels, test_images, test_labels, epochs=100, bs=128, patience=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2dcb5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chebyshev VIT and other architechtures can also be implemented..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
